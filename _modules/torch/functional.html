

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.functional &mdash; NAS Framework v0.1.3 alpha documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="NAS Framework v0.1.3 alpha documentation" href="../../index.html"/>
        <link rel="up" title="torch" href="../torch.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> NAS Framework
          

          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Framework API:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../architect.html">Architect</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../searchspaces.html">Search Spaces</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../searchspaces.html#base">Base</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../searchspaces.html#multilayer-perceptron">Multilayer Perceptron</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../searchspaces.html#recurrent-neural-network">Recurrent Neural Network</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../coaches.html">Training Classes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../coaches.html#base">Base</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../coaches.html#architect">Architect</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../coaches.html#feed-forward">Feed Forward</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../coaches.html#recurrent">Recurrent</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../storage.html">Storage</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../storage.html#plain-storage">Plain Storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../storage.html#curriculum-storage">Curriculum Storage</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../utils.html">Utility functions and classes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../utils.html#module-nasframe.utils.torch">Torch-related</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../utils.html#module-nasframe.utils.filelock">Miscellaneous</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../loaders.html">Data Loaders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../loaders.html#text-loader">Text loader</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../scripts.html">Scripts</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../scripts.html#module-nasframe.scripts.generic">Reusable training functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../scripts.html#module-nasframe.scripts.toxiccomment">Jigsaw Toxic Comment dataset training</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Tutorials:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../config.html">Configuring the search</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../config.html#search-space-section">Search space section</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../config.html#child-training-section">Child training section</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../config.html#batch-size">batch_size</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../config.html#adaptive-batch-size">adaptive_batch_size</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../config.html#min-batch-size">min_batch_size</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../config.html#max-batch-size">max_batch_size</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../config.html#batch-size-decay">batch_size_decay</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../config.html#keep-data-on-device">keep_data_on_device</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../config.html#architect-training-section">Architect training section</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../config.html#curriculum">curriculum</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../config.html#max-curriculum-complexity">max_curriculum_complexity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../config.html#epochs-per-loop">epochs_per_loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../config.html#lr-decay">lr_decay</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../config.html#storage-surplus-factor">storage_surplus_factor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../config.html#load-architect">load_architect</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">NAS Framework</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
          <li><a href="../torch.html">torch</a> &raquo;</li>
        
      <li>torch.functional</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for torch.functional</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="k">import</span> <span class="n">mul</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="k">import</span> <span class="n">reduce</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;argmax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;argmin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bartlett_window&#39;</span><span class="p">,</span>
    <span class="s1">&#39;btrifact&#39;</span><span class="p">,</span>
    <span class="s1">&#39;btriunpack&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hamming_window&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hann_window&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isnan&#39;</span><span class="p">,</span>
    <span class="s1">&#39;split&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unbind&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unique&#39;</span><span class="p">,</span>
<span class="p">]</span>


<div class="viewcode-block" id="split"><a class="viewcode-back" href="../../utils.html#torch.split">[docs]</a><span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">split_size_or_sections</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Splits the tensor into chunks.</span>

<span class="sd">    If :attr:`split_size_or_sections` is an integer type, then :attr:`tensor` will</span>
<span class="sd">    be split into equally sized chunks (if possible). Last chunk will be smaller if</span>
<span class="sd">    the tensor size along the given dimension :attr:`dim= is not divisible by</span>
<span class="sd">    :attr:`split_size`.</span>

<span class="sd">    If :attr:`split_size_or_sections` is a list, then :attr:`tensor` will be split</span>
<span class="sd">    into ``len(split_size_or_sections)`` chunks with sizes in :attr:`dim` according</span>
<span class="sd">    to :attr:`split_size_or_sections`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): tensor to split.</span>
<span class="sd">        split_size_or_sections (int) or (list(int)): size of a single chunk or</span>
<span class="sd">        list of sizes for each chunk</span>
<span class="sd">        dim (int): dimension along which to split the tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Overwriting reason:</span>
    <span class="c1"># This dispatches to two ATen functions depending on the type of</span>
    <span class="c1"># split_size_or_sections. The branching code is in tensor.py, which we</span>
    <span class="c1"># call here.</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">split_size_or_sections</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">btrifact</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pivot</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Batch LU factorization.</span>

<span class="sd">    Returns a tuple containing the LU factorization and pivots. Pivoting is done if</span>
<span class="sd">    :attr:`pivot` is set.</span>

<span class="sd">    The optional argument :attr:`info` stores information if the factorization</span>
<span class="sd">    succeeded for each minibatch example. The :attr:`info` is provided as an</span>
<span class="sd">    `IntTensor`, its values will be filled from dgetrf and a non-zero value</span>
<span class="sd">    indicates an error occurred. Specifically, the values are from cublas if cuda is</span>
<span class="sd">    being used, otherwise LAPACK.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The :attr:`info` argument is deprecated in favor of :meth:`torch.btrifact_with_info`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        A (Tensor): the tensor to factor</span>
<span class="sd">        info (IntTensor, optional): (deprecated) an `IntTensor` to store values</span>
<span class="sd">            indicating whether factorization succeeds</span>
<span class="sd">        pivot (bool, optional): controls whether pivoting is done</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple containing factorization and pivots.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; A = torch.randn(2, 3, 3)</span>
<span class="sd">        &gt;&gt;&gt; A_LU, pivots = torch.btrifact(A)</span>
<span class="sd">        &gt;&gt;&gt; A_LU</span>
<span class="sd">        tensor([[[ 1.3506,  2.5558, -0.0816],</span>
<span class="sd">                 [ 0.1684,  1.1551,  0.1940],</span>
<span class="sd">                 [ 0.1193,  0.6189, -0.5497]],</span>

<span class="sd">                [[ 0.4526,  1.2526, -0.3285],</span>
<span class="sd">                 [-0.7988,  0.7175, -0.9701],</span>
<span class="sd">                 [ 0.2634, -0.9255, -0.3459]]])</span>

<span class="sd">        &gt;&gt;&gt; pivots</span>
<span class="sd">        tensor([[ 3,  3,  3],</span>
<span class="sd">                [ 3,  3,  3]], dtype=torch.int32)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Overwriting reason:</span>
    <span class="c1"># `info` is being deprecated in favor of `btrifact_with_info`. This warning</span>
    <span class="c1"># is in tensor.py, which we call here.</span>
    <span class="k">return</span> <span class="n">A</span><span class="o">.</span><span class="n">btrifact</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">pivot</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">unbind</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Removes a tensor dimension.</span>

<span class="sd">    Returns a tuple of all slices along a given dimension, already without it.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): the tensor to unbind</span>
<span class="sd">        dim (int): dimension to remove</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">dim</span><span class="p">)))</span>


<span class="k">def</span> <span class="nf">btriunpack</span><span class="p">(</span><span class="n">LU_data</span><span class="p">,</span> <span class="n">LU_pivots</span><span class="p">,</span> <span class="n">unpack_data</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unpack_pivots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Unpacks the data and pivots from a batched LU factorization (btrifact) of a tensor.</span>

<span class="sd">    Returns a tuple of tensors as ``(the pivots, the L tensor, the U tensor)``.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        LU_data (Tensor): the packed LU factorization data</span>
<span class="sd">        LU_pivots (Tensor): the packed LU factorization pivots</span>
<span class="sd">        unpack_data (bool): flag indicating if the data should be unpacked</span>
<span class="sd">        unpack_pivots (bool): flag indicating if the pivots should be unpacked</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; A = torch.randn(2, 3, 3)</span>
<span class="sd">        &gt;&gt;&gt; A_LU, pivots = A.btrifact()</span>
<span class="sd">        &gt;&gt;&gt; P, A_L, A_U = torch.btriunpack(A_LU, pivots)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # can recover A from factorization</span>
<span class="sd">        &gt;&gt;&gt; A_ = torch.bmm(P, torch.bmm(A_L, A_U))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">nBatch</span><span class="p">,</span> <span class="n">sz</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">LU_data</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">unpack_data</span><span class="p">:</span>
        <span class="n">I_U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">sz</span><span class="p">,</span> <span class="n">sz</span><span class="p">))</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">LU_data</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">nBatch</span><span class="p">,</span> <span class="n">sz</span><span class="p">,</span> <span class="n">sz</span><span class="p">)</span>
        <span class="n">I_L</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">I_U</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">LU_data</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">LU_data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">LU_data</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">LU_data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="n">I_diag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">sz</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">LU_data</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">nBatch</span><span class="p">,</span> <span class="n">sz</span><span class="p">,</span> <span class="n">sz</span><span class="p">)</span>
        <span class="n">L</span><span class="p">[</span><span class="n">I_diag</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">L</span><span class="p">[</span><span class="n">I_L</span><span class="p">]</span> <span class="o">=</span> <span class="n">LU_data</span><span class="p">[</span><span class="n">I_L</span><span class="p">]</span>
        <span class="n">U</span><span class="p">[</span><span class="n">I_U</span><span class="p">]</span> <span class="o">=</span> <span class="n">LU_data</span><span class="p">[</span><span class="n">I_U</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">U</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">unpack_pivots</span><span class="p">:</span>
        <span class="n">P</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">sz</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">LU_data</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">nBatch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nBatch</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sz</span><span class="p">):</span>
                <span class="n">k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">LU_pivots</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">t</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="n">P</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">k</span><span class="p">]</span>
                <span class="n">P</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">t</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">P</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">P</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">U</span>


<span class="k">def</span> <span class="nf">hann_window</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="n">periodic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Hann window function.</span>

<span class="sd">    This method computes the Hann window function:</span>

<span class="sd">    .. math::</span>
<span class="sd">        w[n] = \frac{1}{2}\ \left[1 - \cos \left( \frac{2 \pi n}{N - 1} \right)\right] =</span>
<span class="sd">                \sin^2 \left( \frac{\pi n}{N - 1} \right),</span>

<span class="sd">    where :math:`N` is the full window size.</span>

<span class="sd">    The input :attr:`window_length` is a positive integer controlling the</span>
<span class="sd">    returned window size. :attr:`periodic` flag determines whether the returned</span>
<span class="sd">    window trims off the last duplicate value from the symmetric window and is</span>
<span class="sd">    ready to be used as a periodic window with functions like</span>
<span class="sd">    :meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in</span>
<span class="sd">    above formula is in fact :math:`\text{window_length} + 1`. Also, we always have</span>
<span class="sd">    ``torch.hann_window(L, periodic=True)`` equal to</span>
<span class="sd">    ``torch.hann_window(L + 1, periodic=False)[:-1])``.</span>

<span class="sd">    .. note::</span>
<span class="sd">        If :attr:`window_length` :math:`=1`, the returned window contains a single value 1.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        window_length (int): the size of returned window</span>
<span class="sd">        periodic (bool, optional): If True, returns a window to be used as periodic</span>
<span class="sd">            function. If False, return a symmetric window.</span>
<span class="sd">        dtype (:class:`torch.dtype`, optional): the desired type of returned window.</span>
<span class="sd">            Default: `torch.float32`</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: A 1-D tensor of size :math:`(\text{window_length},)` containing the window</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dtype must be a floating point type, but got dtype=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">window_length</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;window_length must be positive&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hamming_window</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="n">periodic</span><span class="o">=</span><span class="n">periodic</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">hamming_window</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="n">periodic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.54</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.46</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Hamming window function.</span>

<span class="sd">    This method computes the Hamming window function:</span>

<span class="sd">    .. math::</span>
<span class="sd">        w[n] = \alpha - \beta\ \cos \left( \frac{2 \pi n}{N - 1} \right),</span>

<span class="sd">    where :math:`N` is the full window size.</span>

<span class="sd">    The input :attr:`window_length` is a positive integer controlling the</span>
<span class="sd">    returned window size. :attr:`periodic` flag determines whether the returned</span>
<span class="sd">    window trims off the last duplicate value from the symmetric window and is</span>
<span class="sd">    ready to be used as a periodic window with functions like</span>
<span class="sd">    :meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in</span>
<span class="sd">    above formula is in fact :math:`\text{window_length} + 1`. Also, we always have</span>
<span class="sd">    ``torch.hamming_window(L, periodic=True)`` equal to</span>
<span class="sd">    ``torch.hamming_window(L + 1, periodic=False)[:-1])``.</span>

<span class="sd">    .. note::</span>
<span class="sd">        If :attr:`window_length` :math:`=1`, the returned window contains a single value 1.</span>

<span class="sd">    .. note::</span>
<span class="sd">        This is a generalized version of :meth:`torch.hann_window`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        window_length (int): the size of returned window</span>
<span class="sd">        periodic (bool, optional): If True, returns a window to be used as periodic</span>
<span class="sd">            function. If False, return a symmetric window.</span>
<span class="sd">        dtype (:class:`torch.dtype`, optional): the desired type of returned window.</span>
<span class="sd">            Default: `torch.float32`</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: A 1-D tensor of size :math:`(\text{window_length},)` containing the window</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dtype must be a floating point type, but got dtype=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">window_length</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;window_length must be positive&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">window_length</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">window_length</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">periodic</span><span class="p">)</span>
    <span class="n">window</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">window</span> <span class="o">=</span> <span class="n">window</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">window_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">cos_</span><span class="p">()</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="o">-</span><span class="n">beta</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">periodic</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">window</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">window</span>


<span class="k">def</span> <span class="nf">bartlett_window</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="n">periodic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Bartlett window function.</span>

<span class="sd">    This method computes the Bartlett window function:</span>

<span class="sd">    .. math::</span>
<span class="sd">        w[n] = 1 - \left| \frac{2n}{N-1} - 1 \right| = \begin{cases}</span>
<span class="sd">            \frac{2n}{N - 1} &amp; \text{if } 0 \leq n \leq \frac{N - 1}{2} \\</span>
<span class="sd">            2 - \frac{2n}{N - 1} &amp; \text{if } \frac{N - 1}{2} &lt; n &lt; N \\</span>
<span class="sd">        \end{cases},</span>

<span class="sd">    where :math:`N` is the full window size.</span>

<span class="sd">    The input :attr:`window_length` is a positive integer controlling the</span>
<span class="sd">    returned window size. :attr:`periodic` flag determines whether the returned</span>
<span class="sd">    window trims off the last duplicate value from the symmetric window and is</span>
<span class="sd">    ready to be used as a periodic window with functions like</span>
<span class="sd">    :meth:`torch.stft`. Therefore, if :attr:`periodic` is true, the :math:`N` in</span>
<span class="sd">    above formula is in fact :math:`\text{window_length} + 1`. Also, we always have</span>
<span class="sd">    ``torch.bartlett_window(L, periodic=True)`` equal to</span>
<span class="sd">    ``torch.bartlett_window(L + 1, periodic=False)[:-1])``.</span>

<span class="sd">    .. note::</span>
<span class="sd">        If :attr:`window_length` :math:`=1`, the returned window contains a single value 1.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        window_length (int): the size of returned window</span>
<span class="sd">        periodic (bool, optional): If True, returns a window to be used as periodic</span>
<span class="sd">            function. If False, return a symmetric window.</span>
<span class="sd">        dtype (:class:`torch.dtype`, optional): the desired type of returned window.</span>
<span class="sd">            Default: `torch.float32`</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: A 1-D tensor of size :math:`(\text{window_length},)` containing the window</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dtype must be a floating point type, but got dtype=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">window_length</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;window_length must be positive&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">window_length</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">window_length</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">periodic</span><span class="p">)</span>
    <span class="n">window</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">window_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">first_half_size</span> <span class="o">=</span> <span class="p">((</span><span class="n">window_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">window</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">first_half_size</span><span class="p">,</span> <span class="n">window_length</span> <span class="o">-</span> <span class="n">first_half_size</span><span class="p">)</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">periodic</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">window</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">window</span>


<span class="k">def</span> <span class="nf">isnan</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a new tensor with boolean elements representing if each element is `NaN` or not.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): A tensor to check</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: A ``torch.ByteTensor`` containing a 1 at each location of `NaN` elements.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; torch.isnan(torch.tensor([1, float(&#39;nan&#39;), 2]))</span>
<span class="sd">        tensor([ 0,  1,  0], dtype=torch.uint8)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The argument is not a tensor&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor</span> <span class="o">!=</span> <span class="n">tensor</span>


<span class="k">def</span> <span class="nf">unique</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the unique scalar elements of the input tensor as a 1-D tensor.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input (Tensor): the input tensor</span>
<span class="sd">        sorted (bool): Whether to sort the unique elements in ascending order</span>
<span class="sd">            before returning as output.</span>
<span class="sd">        return_inverse (bool): Whether to also return the indices for where</span>
<span class="sd">            elements in the original input ended up in the returned unique list.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (Tensor, Tensor (optional)): A tensor or a tuple of tensors containing</span>

<span class="sd">            - **output** (*Tensor*): the output list of unique scalar elements.</span>
<span class="sd">            - **inverse_indices** (*Tensor*): (optional) if</span>
<span class="sd">              :attr:`return_inverse` is True, there will be a</span>
<span class="sd">              2nd returned tensor (same shape as input) representing the indices</span>
<span class="sd">              for where elements in the original input map to in the output;</span>
<span class="sd">              otherwise, this function will only return a single tensor.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        tensor([ 2,  3,  1])</span>

<span class="sd">        &gt;&gt;&gt; output, inverse_indices = torch.unique(</span>
<span class="sd">                torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        tensor([ 1,  2,  3])</span>
<span class="sd">        &gt;&gt;&gt; inverse_indices</span>
<span class="sd">        tensor([ 0,  2,  1,  2])</span>

<span class="sd">        &gt;&gt;&gt; output, inverse_indices = torch.unique(</span>
<span class="sd">                torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        tensor([ 1,  2,  3])</span>
<span class="sd">        &gt;&gt;&gt; inverse_indices</span>
<span class="sd">        tensor([[ 0,  2],</span>
<span class="sd">                [ 1,  2]])</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">inverse_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_unique</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="nb">sorted</span><span class="o">=</span><span class="nb">sorted</span><span class="p">,</span>
        <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">return_inverse</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">inverse_indices</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span>


<span class="k">def</span> <span class="nf">argmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the indices of the maximum values of a tensor across a dimension.</span>

<span class="sd">    This is the second value returned by :meth:`torch.max`. See its</span>
<span class="sd">    documentation for the exact semantics of this method.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): the input tensor</span>
<span class="sd">        dim (int): the dimension to reduce. If ``None``, the argmax of the</span>
<span class="sd">            flattened input is returned.</span>
<span class="sd">        keepdim (bool): whether the output tensors have :attr:`dim`</span>
<span class="sd">            retained or not. Ignored if ``dim=None``.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; a = torch.randn(4, 4)</span>
<span class="sd">        &gt;&gt;&gt; a</span>
<span class="sd">        tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],</span>
<span class="sd">                [-0.7401, -0.8805, -0.3402, -1.1936],</span>
<span class="sd">                [ 0.4907, -1.3948, -1.0691, -0.3132],</span>
<span class="sd">                [-1.6092,  0.5419, -0.2993,  0.3195]])</span>


<span class="sd">        &gt;&gt;&gt; torch.argmax(a, dim=1)</span>
<span class="sd">        tensor([ 0,  2,  0,  1])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_argmax</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_argmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">argmin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the indices of the minimum values of a tensor across a dimension.</span>

<span class="sd">    This is the second value returned by :meth:`torch.min`. See its</span>
<span class="sd">    documentation for the exact semantics of this method.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): the input tensor</span>
<span class="sd">        dim (int): the dimension to reduce. If ``None``, the argmin of the</span>
<span class="sd">            flattened input is returned.</span>
<span class="sd">        keepdim (bool): whether the output tensors have :attr:`dim`</span>
<span class="sd">            retained or not. Ignored if ``dim=None``.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; a = torch.randn(4, 4)</span>
<span class="sd">        &gt;&gt;&gt; a</span>
<span class="sd">        tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],</span>
<span class="sd">                [ 1.0100, -1.1975, -0.0102, -0.4732],</span>
<span class="sd">                [-0.9240,  0.1207, -0.7506, -1.0213],</span>
<span class="sd">                [ 1.7809, -1.2960,  0.9384,  0.1438]])</span>


<span class="sd">        &gt;&gt;&gt; torch.argmin(a, dim=1)</span>
<span class="sd">        tensor([ 2,  1,  3,  1])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_argmin</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_argmin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">)</span>
</pre></div>

           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Vladislav Zavadskyy.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'v0.1.3 alpha',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>